{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnnLO1v1+tFuQWZsODtnps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DilemmaFixer3/AI_pr_5-6-7/blob/main/pr8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lYlddntuorA",
        "outputId": "11f6e056-cf26-4beb-d804-2ecf3ee5ffaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " –í–∏–¥–∞–ª—è—î–º–æ 42 —Ä—ñ–¥–∫—ñ—Å–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤ (—Ö–≤–æ—Ä–æ–±), —â–æ –º–∞—é—Ç—å < 2 –∑—Ä–∞–∑–∫—ñ–≤.\n",
            " –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ. –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: 682\n",
            "---  –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö: 377 –æ–∑–Ω–∞–∫, 682 –∫–ª–∞—Å—ñ–≤ ---\n",
            "---  –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ 20 –µ–ø–æ—Ö–∞—Ö ---\n",
            "\n",
            "## 1. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è üß™\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: SGD, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.0001\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: SGD, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.001\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: SGD, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.01\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: Adam, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.0001\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: Adam, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.001\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: Adam, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.01\n",
            "| Optimizer   |     LR | Train Accuracy   | Test Accuracy   |\n",
            "|:------------|-------:|:-----------------|:----------------|\n",
            "| SGD         | 0.0001 | 0.53%            | 0.76%           |\n",
            "| SGD         | 0.001  | 3.20%            | 6.50%           |\n",
            "| SGD         | 0.01   | 64.72%           | 79.08%          |\n",
            "| Adam        | 0.0001 | 48.32%           | 70.23%          |\n",
            "| Adam        | 0.001  | 75.81%           | 83.66%          |\n",
            "| Adam        | 0.01   | 64.83%           | 79.54%          |\n",
            "\n",
            "## 2. –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤: –í–ª–∞—Å–Ω–∞ –≤–µ—Ä—Å—ñ—è Adam (CustomAdam) \n",
            "-> CustomAdam (LR: 0.001)\n",
            "-> –í–±—É–¥–æ–≤–∞–Ω–∏–π Adam (LR: 0.001)\n",
            "| Optimizer     |    LR | Test Accuracy   |\n",
            "|:--------------|------:|:----------------|\n",
            "| CustomAdam    | 0.001 | 84.05%          |\n",
            "| Built-in Adam | 0.001 | 83.38%          |\n",
            "\n",
            "## 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –Ω–æ–≤–∏–º–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞–º–∏ (Nadam, AdamW)\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: NAdam, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.001\n",
            "-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: AdamW, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): 0.001\n",
            "| Optimizer     |    LR | Test Accuracy   |\n",
            "|:--------------|------:|:----------------|\n",
            "| NAdam         | 0.001 | 83.92%          |\n",
            "| AdamW         | 0.001 | 83.86%          |\n",
            "| Built-in Adam | 0.001 | 83.38%          |\n",
            "\n",
            "## 4. –ê–Ω–∞–ª—ñ–∑ –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏—Ö –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö \n",
            "> –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—è –Ω–∞ 20% —Ä–µ–∞–ª—å–Ω–∏—Ö –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö,\n",
            "> —è–∫ –∑–∞–∑–Ω–∞—á–µ–Ω–æ —É –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö (X_train, X_test, y_train, y_test).\n",
            "> –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ —É 20% –≤–∏–±—ñ—Ä—Ü—ñ: 49347.\n",
            "\n",
            "-> –§—ñ–Ω–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ AdamW:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "–§—ñ–Ω–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:54<00:00,  2.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "** –§—ñ–Ω–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ (AdamW, LR=1e-3): 83.82%**\n",
            "** –§—ñ–Ω–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ: 0.4520**\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm # –î–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É\n",
        "\n",
        "# –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Ñ—ñ–∫—Å–æ–≤–∞–Ω–µ –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- 0. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö ---\n",
        "\n",
        "def load_and_preprocess_data(file_path, sample_frac=0.2):\n",
        "    \"\"\"\n",
        "    –û–Ω–æ–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è: –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î, –∑–º–µ–Ω—à—É—î, –≤–∏–¥–∞–ª—è—î —Ä—ñ–¥–∫—ñ—Å–Ω—ñ –∫–ª–∞—Å–∏\n",
        "    —Ç–∞ —Ä–æ–∑–¥—ñ–ª—è—î –¥–∞–Ω—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —Å—Ç—Ä–∞—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        # –ó–º–µ–Ω—à–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –¥–æ 20%\n",
        "        df = df.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª '{file_path}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "    disease_col = next((col for col in df.columns if 'disease' in col.lower()), None)\n",
        "    if not disease_col:\n",
        "        print(\"–ü–æ–º–∏–ª–∫–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ —Å—Ç–æ–≤–ø–µ—Ü—å –∑ –Ω–∞–∑–≤–æ—é —Ö–≤–æ—Ä–æ–±–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'Disease').\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    X = df.drop(columns=[disease_col])\n",
        "    y_labels = df[disease_col]\n",
        "\n",
        "    # --- –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –ü–û–ú–ò–õ–ö–ò: –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤ –∑ –æ–¥–Ω–∏–º –∑—Ä–∞–∑–∫–æ–º ---\n",
        "    class_counts = y_labels.value_counts()\n",
        "    rare_classes = class_counts[class_counts < 2].index.tolist()\n",
        "\n",
        "    if rare_classes:\n",
        "        print(f\" –í–∏–¥–∞–ª—è—î–º–æ {len(rare_classes)} —Ä—ñ–¥–∫—ñ—Å–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤ (—Ö–≤–æ—Ä–æ–±), —â–æ –º–∞—é—Ç—å < 2 –∑—Ä–∞–∑–∫—ñ–≤.\")\n",
        "        df_filtered = df[~df[disease_col].isin(rare_classes)]\n",
        "\n",
        "        X = df_filtered.drop(columns=[disease_col])\n",
        "        y_labels = df_filtered[disease_col]\n",
        "\n",
        "        if X.empty:\n",
        "            print(\"–ü–æ–º–∏–ª–∫–∞: –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–µ –∑–∞–ª–∏—à–∏–ª–æ—Å—è –¥–∞–Ω–∏—Ö.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –º—ñ—Ç–æ–∫ (—Ö–≤–æ—Ä–æ–±) –Ω–∞ —á–∏—Å–ª–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y_labels)\n",
        "\n",
        "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞ —Ç–µ–Ω–∑–æ—Ä–∏\n",
        "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —Ç–∞ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏ (—Å—Ç—Ä–∞—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–µ–ø–µ—Ä –º–æ–∂–ª–∏–≤–∞)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor\n",
        "    )\n",
        "\n",
        "    print(f\" –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ. –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {len(label_encoder.classes_)}\")\n",
        "    return X_train, X_test, y_train, y_test, len(label_encoder.classes_)\n",
        "\n",
        "# –í–∫–∞–∂—ñ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É\n",
        "FILE_PATH = 'Final_Augmented_dataset_Diseases_and_Symptoms.csv' # –ó–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ —ñ–º'—è –≤–∞—à–æ–≥–æ —Ñ–∞–π–ª—É\n",
        "X_train, X_test, y_train, y_test, NUM_CLASSES = load_and_preprocess_data(FILE_PATH, sample_frac=0.2)\n",
        "\n",
        "if X_train is None:\n",
        "    exit()\n",
        "\n",
        "INPUT_SIZE = X_train.shape[1]\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 20 # –ó–º–µ–Ω—à–∏–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è\n",
        "\n",
        "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤ Dataset —Ç–∞ DataLoader\n",
        "class SymptomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SymptomDataset(X_train, y_train)\n",
        "test_dataset = SymptomDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- 1. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ ---\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)\n",
        "\n",
        "# --- 2. –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤: –í–ª–∞—Å–Ω–∞ –≤–µ—Ä—Å—ñ—è Adam ---\n",
        "\n",
        "class CustomAdam(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    –í–ª–∞—Å–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ Adam –∑ —Ä–µ–≥—É–ª—å–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
        "    –ù–∞—Å–ª—ñ–¥—É—î torch.optim.Optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è epsilon: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è beta1: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è beta2: {betas[1]}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è weight_decay: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"–í–∏–∫–æ–Ω—É—î —î–¥–∏–Ω–∏–π –∫—Ä–æ–∫ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î —Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å—Ç–∞–Ω—É (–º–æ–º–µ–Ω—Ç—ñ–≤)\n",
        "                if not state:\n",
        "                    state['step'] = 0\n",
        "                    # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ-–∑–≤–∞–∂–µ–Ω–µ —Å–µ—Ä–µ–¥–Ω—î –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ-–∑–≤–∞–∂–µ–Ω–µ —Å–µ—Ä–µ–¥–Ω—î –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # 1. Weight decay (L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è) - –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è —è–∫ —É AdamW –¥–ª—è –∫—Ä–∞—â–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ\n",
        "                # –£ –∫–ª–∞—Å–∏—á–Ω–æ–º—É Adam –π–æ–≥–æ –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å –¥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞, –∞–ª–µ —Ç—É—Ç —Ä–æ–±–∏–º–æ AdamW-–ø–æ–¥—ñ–±–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                # 2. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–µ—Ä—à–æ–≥–æ –º–æ–º–µ–Ω—Ç—É (exp_avg)\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                # 3. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –¥—Ä—É–≥–æ–≥–æ –º–æ–º–µ–Ω—Ç—É (exp_avg_sq)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # 4. –ö–æ—Ä–µ–∫—Ü—ñ—è –∑–º—ñ—â–µ–Ω–Ω—è (Bias correction)\n",
        "                denom = (exp_avg_sq.sqrt() / np.sqrt(bias_correction2)).add_(group['eps'])\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "\n",
        "                # 5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# --- 3. –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ ---\n",
        "\n",
        "def train_model(model, optimizer, criterion, train_loader, n_epochs):\n",
        "    \"\"\"–¢—Ä–µ–Ω—É—î –º–æ–¥–µ–ª—å —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    \"\"\"–û—Ü—ñ–Ω—é—î –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# --- 4. –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π —Ü–∏–∫–ª –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å ---\n",
        "\n",
        "def run_experiment(model_class, input_size, num_classes, train_loader, test_loader,\n",
        "                   optimizer_config, n_epochs):\n",
        "    \"\"\"–ü—Ä–æ–≤–æ–¥–∏—Ç—å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –∑–∞–¥–∞–Ω–∏–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–æ–º —ñ —à–≤–∏–¥–∫—ñ—Å—Ç—é –Ω–∞–≤—á–∞–Ω–Ω—è.\"\"\"\n",
        "\n",
        "    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ —Ç–∞ –∫—Ä–∏—Ç–µ—Ä—ñ—é –≤—Ç—Ä–∞—Ç\n",
        "    model = model_class(input_size, num_classes)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞\n",
        "    optim_name = optimizer_config['name']\n",
        "    lr = optimizer_config['lr']\n",
        "\n",
        "    if optim_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif optim_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optim_name == 'CustomAdam':\n",
        "        optimizer = CustomAdam(model.parameters(), lr=lr)\n",
        "    elif optim_name == 'NAdam':\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
        "    elif optim_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(f\"–ù–µ–≤—ñ–¥–æ–º–∏–π –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: {optim_name}\")\n",
        "\n",
        "    # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train_model(model, optimizer, criterion, train_loader, 1)\n",
        "        # print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # –û—Ü—ñ–Ω–∫–∞\n",
        "    test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "    return train_acc, test_acc, test_loss\n",
        "\n",
        "\n",
        "# --- –ì–æ–ª–æ–≤–Ω–∏–π –±–ª–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å ---\n",
        "def main_lab_work():\n",
        "\n",
        "    print(f\"---  –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö: {INPUT_SIZE} –æ–∑–Ω–∞–∫, {NUM_CLASSES} –∫–ª–∞—Å—ñ–≤ ---\")\n",
        "    print(f\"---  –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ {N_EPOCHS} –µ–ø–æ—Ö–∞—Ö ---\")\n",
        "\n",
        "    # 1. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è (Learning Rate)\n",
        "    print(\"\\n## 1. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è üß™\")\n",
        "\n",
        "    optimizers_to_test = ['SGD', 'Adam']\n",
        "    learning_rates = [1e-4, 1e-3, 1e-2]\n",
        "\n",
        "    results_lr = []\n",
        "\n",
        "    for opt_name in optimizers_to_test:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: {opt_name}, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): {lr}\")\n",
        "            config = {'name': opt_name, 'lr': lr}\n",
        "\n",
        "            # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É\n",
        "            train_acc, test_acc, _ = run_experiment(\n",
        "                SimpleClassifier, INPUT_SIZE, NUM_CLASSES,\n",
        "                train_loader, test_loader, config, N_EPOCHS\n",
        "            )\n",
        "\n",
        "            results_lr.append({\n",
        "                'Optimizer': opt_name,\n",
        "                'LR': lr,\n",
        "                'Train Accuracy': f\"{train_acc:.2f}%\",\n",
        "                'Test Accuracy': f\"{test_acc:.2f}%\"\n",
        "            })\n",
        "\n",
        "    print(pd.DataFrame(results_lr).to_markdown(index=False))\n",
        "\n",
        "\n",
        "    # 2. –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤: –í–ª–∞—Å–Ω–∞ –≤–µ—Ä—Å—ñ—è Adam\n",
        "    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ CustomAdam –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º LR, –∑–Ω–∞–π–¥–µ–Ω–∏–º —É –ø.1 (–∞–±–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–º 1e-3)\n",
        "    print(\"\\n## 2. –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤: –í–ª–∞—Å–Ω–∞ –≤–µ—Ä—Å—ñ—è Adam (CustomAdam) \")\n",
        "\n",
        "    # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è CustomAdam –∑ –≤–±—É–¥–æ–≤–∞–Ω–∏–º Adam\n",
        "    best_lr_adam = 1e-3\n",
        "\n",
        "    custom_adam_config = {'name': 'CustomAdam', 'lr': best_lr_adam}\n",
        "    print(f\"-> CustomAdam (LR: {best_lr_adam})\")\n",
        "    custom_train_acc, custom_test_acc, _ = run_experiment(\n",
        "        SimpleClassifier, INPUT_SIZE, NUM_CLASSES,\n",
        "        train_loader, test_loader, custom_adam_config, N_EPOCHS\n",
        "    )\n",
        "\n",
        "    adam_config = {'name': 'Adam', 'lr': best_lr_adam}\n",
        "    print(f\"-> –í–±—É–¥–æ–≤–∞–Ω–∏–π Adam (LR: {best_lr_adam})\")\n",
        "    adam_train_acc, adam_test_acc, _ = run_experiment(\n",
        "        SimpleClassifier, INPUT_SIZE, NUM_CLASSES,\n",
        "        train_loader, test_loader, adam_config, N_EPOCHS\n",
        "    )\n",
        "\n",
        "    results_custom = [{\n",
        "        'Optimizer': 'CustomAdam',\n",
        "        'LR': best_lr_adam,\n",
        "        'Test Accuracy': f\"{custom_test_acc:.2f}%\"\n",
        "    }, {\n",
        "        'Optimizer': 'Built-in Adam',\n",
        "        'LR': best_lr_adam,\n",
        "        'Test Accuracy': f\"{adam_test_acc:.2f}%\"\n",
        "    }]\n",
        "\n",
        "    print(pd.DataFrame(results_custom).to_markdown(index=False))\n",
        "\n",
        "\n",
        "    # 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –Ω–æ–≤–∏–º–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞–º–∏ (Nadam, AdamW)\n",
        "    print(\"\\n## 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –Ω–æ–≤–∏–º–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞–º–∏ (Nadam, AdamW)\")\n",
        "\n",
        "    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π LR (1e-3)\n",
        "    new_optimizers = ['NAdam', 'AdamW']\n",
        "    default_lr = 1e-3\n",
        "\n",
        "    results_new_opt = []\n",
        "\n",
        "    for opt_name in new_optimizers:\n",
        "        print(f\"-> –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: {opt_name}, –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (LR): {default_lr}\")\n",
        "        config = {'name': opt_name, 'lr': default_lr}\n",
        "\n",
        "        train_acc, test_acc, _ = run_experiment(\n",
        "            SimpleClassifier, INPUT_SIZE, NUM_CLASSES,\n",
        "            train_loader, test_loader, config, N_EPOCHS\n",
        "        )\n",
        "\n",
        "        results_new_opt.append({\n",
        "            'Optimizer': opt_name,\n",
        "            'LR': default_lr,\n",
        "            'Test Accuracy': f\"{test_acc:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # –î–æ–¥–∞–º–æ Adam –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è\n",
        "    results_new_opt.append({\n",
        "        'Optimizer': 'Built-in Adam',\n",
        "        'LR': default_lr,\n",
        "        'Test Accuracy': f\"{adam_test_acc:.2f}%\"\n",
        "    })\n",
        "\n",
        "    print(pd.DataFrame(results_new_opt).to_markdown(index=False))\n",
        "\n",
        "\n",
        "    # 4. –ê–Ω–∞–ª—ñ–∑ –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏—Ö –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö\n",
        "    print(\"\\n## 4. –ê–Ω–∞–ª—ñ–∑ –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏—Ö –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö \")\n",
        "    print(\"> –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—è –Ω–∞ 20% —Ä–µ–∞–ª—å–Ω–∏—Ö –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö,\")\n",
        "    print(\"> —è–∫ –∑–∞–∑–Ω–∞—á–µ–Ω–æ —É –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö (X_train, X_test, y_train, y_test).\")\n",
        "    print(f\"> –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ —É 20% –≤–∏–±—ñ—Ä—Ü—ñ: {X_train.shape[0] + X_test.shape[0]}.\")\n",
        "\n",
        "    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â–∏–π –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –∑ —É—Å—ñ—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, AdamW)\n",
        "    best_opt_config = {'name': 'AdamW', 'lr': 1e-3}\n",
        "    final_model = SimpleClassifier(INPUT_SIZE, NUM_CLASSES)\n",
        "    final_optimizer = optim.AdamW(final_model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n-> –§—ñ–Ω–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ AdamW:\")\n",
        "\n",
        "    # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä–æ–º\n",
        "    for epoch in tqdm(range(N_EPOCHS), desc=\"–§—ñ–Ω–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\"):\n",
        "        train_model(final_model, final_optimizer, criterion, train_loader, 1)\n",
        "\n",
        "    final_test_loss, final_test_acc = evaluate_model(final_model, test_loader, criterion)\n",
        "\n",
        "    print(f\"\\n** –§—ñ–Ω–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ (AdamW, LR=1e-3): {final_test_acc:.2f}%**\")\n",
        "    print(f\"** –§—ñ–Ω–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ: {final_test_loss:.4f}**\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_lab_work()"
      ]
    }
  ]
}