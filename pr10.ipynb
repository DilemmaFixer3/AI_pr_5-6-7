{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyMBV8yfUw5hdCeaWDwYR1gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DilemmaFixer3/AI_pr_5-6-7/blob/main/pr10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Розпакування архіву\n",
        "!unzip -q пр_10.zip\n",
        "print(\"Архів пр_10.zip успішно розпаковано. Створені папки: Train, Test, Meta.\")"
      ],
      "metadata": {
        "id": "0-llWL8hcJNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5eNKu8sYOms"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from skimage.util import random_noise\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Глобальні Параметри (Переконайтесь, що ці значення правильні для вашої підмножини!) ---\n",
        "DATA_DIR_TRAIN = './Train'  # Шлях до розпакованої папки Train\n",
        "DATA_DIR_TEST = './Test'    # Шлях до розпакованої папки Test\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "# Якщо ви використовуєте підмножину з 5 класів, змініть NUM_CLASSES на 5\n",
        "NUM_CLASSES = 5\n",
        "VALIDATION_SPLIT = 0.2  # 20% від Train піде на Val\n",
        "\n",
        "# Відключення попереджень\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Функції Препроцесингу ---\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "def preprocess_dataset(ds, model_type='resnet'):\n",
        "    if model_type == 'resnet':\n",
        "        preprocess_fn = resnet_preprocess\n",
        "    else:\n",
        "        preprocess_fn = mobilenet_preprocess\n",
        "\n",
        "    # Застосовуємо аугментацію лише до навчальних даних\n",
        "    if model_type == 'train':\n",
        "         ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "    # Застосовуємо специфічну попередню обробку\n",
        "    return ds.map(lambda x, y: (preprocess_fn(x), y)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# --- 1. Завантаження та Розділення Навчального Набору ---\n",
        "print(\"Завантаження та розділення Train на Train/Val...\")\n",
        "\n",
        "# Завантажуємо весь навчальний набір\n",
        "full_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR_TRAIN,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "CLASS_NAMES = full_train_ds.class_names\n",
        "print(f\"Знайдено класів: {len(CLASS_NAMES)}\")\n",
        "\n",
        "# Розділення на Train та Val\n",
        "val_size = int(len(full_train_ds) * VALIDATION_SPLIT)\n",
        "train_size = len(full_train_ds) - val_size\n",
        "\n",
        "train_ds = full_train_ds.take(train_size)\n",
        "val_ds = full_train_ds.skip(train_size).take(val_size)\n",
        "\n",
        "# --- 2. Завантаження Тестового Набору ---\n",
        "test_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR_TEST,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- 3. Фінальні Набори Даних з Препроцесингом ---\n",
        "train_ds_res = preprocess_dataset(train_ds, 'resnet')\n",
        "val_ds_res = preprocess_dataset(val_ds, 'resnet')\n",
        "test_ds_res = preprocess_dataset(test_ds_raw, 'resnet')\n",
        "\n",
        "train_ds_mob = preprocess_dataset(train_ds, 'mobilenet')\n",
        "val_ds_mob = preprocess_dataset(val_ds, 'mobilenet')\n",
        "test_ds_mob = preprocess_dataset(test_ds_raw, 'mobilenet')\n",
        "\n",
        "print(\"Підготовка даних завершена.\")"
      ],
      "metadata": {
        "id": "W4inhSnCbOKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Функція 1: Створення Моделі ---\n",
        "def create_model(base_model_class, input_shape, num_classes):\n",
        "    base_model = base_model_class(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    print(f\"Створено {base_model_class.__name__}. Параметрів: {model.count_params() / 1e6:.2f} млн\")\n",
        "    return model, base_model\n",
        "\n",
        "# --- Функція 2: Навчання та Fine-tuning ---\n",
        "def run_training_experiment(model, base_model, ds_train, ds_val, ds_test, fine_tune_layers, model_name):\n",
        "    print(f\"\\n--- Експеримент {model_name} (Fine-tuning {fine_tune_layers} шарів) ---\")\n",
        "\n",
        "    # 1. Початкове навчання (лише класифікатор)\n",
        "    print(\"Навчання класифікатора...\")\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    start_time_init = time.time()\n",
        "    hist_init = model.fit(ds_train, epochs=5, validation_data=ds_val, verbose=1)\n",
        "\n",
        "    # 2. Fine-tuning\n",
        "    print(\"Розморожування та Fine-tuning...\")\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-fine_tune_layers]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    start_time_ft = time.time()\n",
        "    hist_ft = model.fit(ds_train, epochs=10, validation_data=ds_val, verbose=1)\n",
        "\n",
        "    total_time = (time.time() - start_time_init)\n",
        "\n",
        "    # 3. Збереження та Оцінка\n",
        "    model.save(f'{model_name}_best.h5')\n",
        "    loss, acc = model.evaluate(ds_test, verbose=0)\n",
        "    print(f\"\\n{model_name} Фінальна точність: {acc:.4f} | Час: {total_time:.2f} сек\")\n",
        "\n",
        "    return model, hist_init, hist_ft, total_time, acc\n",
        "\n",
        "\n",
        "# --- ВИКОНАННЯ ЕКСПЕРИМЕНТІВ ---\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "# 1. ResNet-50\n",
        "resnet_model, resnet_base = create_model(ResNet50, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "models['ResNet50'], hist_res_init, hist_res_ft, time_res, acc_res = run_training_experiment(\n",
        "    resnet_model, resnet_base, train_ds_res, val_ds_res, test_ds_res,\n",
        "    fine_tune_layers=20, model_name='ResNet50'\n",
        ")\n",
        "results['ResNet50'] = {'time': time_res, 'accuracy': acc_res, 'params': resnet_model.count_params(), 'history': {'init': hist_res_init, 'ft': hist_res_ft}}\n",
        "\n",
        "\n",
        "# 2. MobileNetV2\n",
        "mobilenet_model, mobilenet_base = create_model(MobileNetV2, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "models['MobileNetV2'], hist_mob_init, hist_mob_ft, time_mob, acc_mob = run_training_experiment(\n",
        "    mobilenet_model, mobilenet_base, train_ds_mob, val_ds_mob, test_ds_mob,\n",
        "    fine_tune_layers=10, model_name='MobileNetV2'\n",
        ")\n",
        "results['MobileNetV2'] = {'time': time_mob, 'accuracy': acc_mob, 'params': mobilenet_model.count_params(), 'history': {'init': hist_mob_init, 'ft': hist_mob_ft}}"
      ],
      "metadata": {
        "id": "j89H_yHPbQwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Ансамбль (Середнє Агрегування) ---\n",
        "print(\"\\n--- Ансамбль Моделей ---\")\n",
        "\n",
        "# Отримання ймовірностей\n",
        "preds_res = models['ResNet50'].predict(test_ds_res, verbose=0)\n",
        "preds_mob = models['MobileNetV2'].predict(test_ds_mob, verbose=0)\n",
        "\n",
        "# Істинні мітки\n",
        "y_true = np.concatenate([y for x, y in test_ds_raw]).argmax(axis=1)\n",
        "\n",
        "# Середнє агрегування\n",
        "preds_avg = (preds_res + preds_mob) / 2\n",
        "ensemble_predictions = np.argmax(preds_avg, axis=1)\n",
        "acc_ensemble = accuracy_score(y_true, ensemble_predictions)\n",
        "\n",
        "print(f\"Точність Ансамблю (Середнє): {acc_ensemble:.4f}\")\n",
        "\n",
        "# --- 2. Побудова Графіків Навчання та CM (для ResNet) ---\n",
        "def plot_training_history(history_init, history_ft, model_name):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Об'єднання історій\n",
        "    full_loss = history_init.history['loss'] + history_ft.history['loss']\n",
        "    full_val_loss = history_init.history['val_loss'] + history_ft.history['val_loss']\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(full_loss, label='Train Loss')\n",
        "    plt.plot(full_val_loss, label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss (Fine-tuning)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# plot_training_history(results['ResNet50']['history']['init'], results['ResNet50']['history']['ft'], 'ResNet50')\n",
        "# plot_training_history(results['MobileNetV2']['history']['init'], results['MobileNetV2']['history']['ft'], 'MobileNetV2')\n",
        "\n",
        "\n",
        "# Матриця Змішування (для ResNet)\n",
        "y_pred_res = np.argmax(preds_res, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred_res)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
        "plt.title('Confusion Matrix for ResNet-50')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZfIQnim1bTbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Квантизація MobileNetV2 ---\n",
        "mobilenet_model_ft = models['MobileNetV2']\n",
        "print(\"\\n--- Квантизація MobileNetV2 ---\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(mobilenet_model_ft)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "quant_file_path = 'mobilenet_quantized.tflite'\n",
        "with open(quant_file_path, 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "# Порівняння розмірів\n",
        "original_size = os.path.getsize('MobileNetV2_best.h5') / (1024 * 1024)\n",
        "quant_size = os.path.getsize(quant_file_path) / (1024 * 1024)\n",
        "print(f\"Розмір Оригінал: {original_size:.2f} MB | Квантований: {quant_size:.2f} MB\")"
      ],
      "metadata": {
        "id": "v54l7L-xbWXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pSoVcN_ct6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vq5rj0U6ctsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifH6NBJpctdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Функції Створення Шуму/Дефектів ---\n",
        "def add_gaussian_noise(image_tensor, var=0.01):\n",
        "    img_np = image_tensor.numpy() / 255.0\n",
        "    sigma = var**0.5\n",
        "    noisy_img = img_np + np.random.normal(0, sigma, img_np.shape)\n",
        "    return tf.convert_to_tensor(np.clip(noisy_img * 255.0, 0, 255).astype(np.float32))\n",
        "\n",
        "def add_salt_and_pepper_noise(image_tensor, amount=0.01):\n",
        "    img_np = image_tensor.numpy() / 255.0\n",
        "    noisy_img = random_noise(img_np, mode='s&p', amount=amount)\n",
        "    return tf.convert_to_tensor(np.clip(noisy_img * 255.0, 0, 255).astype(np.float32))\n",
        "\n",
        "def apply_affine_shear(image_tensor, shear_factor=0.1):\n",
        "    img_np = image_tensor.numpy().astype(np.uint8)\n",
        "    h, w = IMAGE_SIZE\n",
        "\n",
        "    # Матриця зсуву (Shear)\n",
        "    M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n",
        "    M[0, 2] = -shear_factor * h / 2\n",
        "    M[1, 2] = 0\n",
        "\n",
        "    sheared_img = cv2.warpAffine(img_np, M, (w, h))\n",
        "    return tf.convert_to_tensor(sheared_img.astype(np.float32))\n",
        "\n",
        "# --- Функція для Створення Тестового Набору з Дефектами ---\n",
        "def create_defective_ds(ds_original, defect_fn, preprocess_fn):\n",
        "    return ds_original.unbatch().map(\n",
        "        lambda x, y: (defect_fn(x), y),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).batch(BATCH_SIZE).map(\n",
        "        lambda x, y: (preprocess_fn(x), y)\n",
        "    )\n",
        "\n",
        "if resnet_model_ft and mobilenet_model_ft:\n",
        "\n",
        "    # Створення тестових наборів з дефектами (для ResNet-50)\n",
        "    test_gauss_res = create_defective_ds(test_ds, add_gaussian_noise, resnet_preprocess)\n",
        "    test_snp_res = create_defective_ds(test_ds, add_salt_and_pepper_noise, resnet_preprocess)\n",
        "    test_affine_res = create_defective_ds(test_ds, apply_affine_shear, resnet_preprocess)\n",
        "\n",
        "    # Створення тестових наборів з дефектами (для MobileNetV2)\n",
        "    test_gauss_mob = create_defective_ds(test_ds, add_gaussian_noise, mobilenet_preprocess)\n",
        "    test_snp_mob = create_defective_ds(test_ds, add_salt_and_pepper_noise, mobilenet_preprocess)\n",
        "    test_affine_mob = create_defective_ds(test_ds, apply_affine_shear, mobilenet_preprocess)\n",
        "\n",
        "    print(\"\\n--- Дослідження Стійкості ---\")\n",
        "\n",
        "    metrics = {\n",
        "        'ResNet-50': resnet_model_ft,\n",
        "        'MobileNetV2': mobilenet_model_ft\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in metrics.items():\n",
        "        preprocess_fn = resnet_preprocess if name == 'ResNet-50' else mobilenet_preprocess\n",
        "\n",
        "        # Використовуємо відповідні набори даних\n",
        "        test_sets = {\n",
        "            'Original': test_ds_res if name == 'ResNet-50' else test_ds_mob,\n",
        "            'Gaussian Noise': test_gauss_res if name == 'ResNet-50' else test_gauss_mob,\n",
        "            'Salt-and-Pepper': test_snp_res if name == 'ResNet-50' else test_snp_mob,\n",
        "            'Affine Shear': test_affine_res if name == 'ResNet-50' else test_affine_mob,\n",
        "        }\n",
        "\n",
        "        results[name] = {}\n",
        "        print(f\"\\nТестування {name}:\")\n",
        "\n",
        "        for defect_type, ds in test_sets.items():\n",
        "            loss, acc = model.evaluate(ds, verbose=0)\n",
        "            results[name][defect_type] = acc\n",
        "            print(f\"  {defect_type}: {acc:.4f}\")\n",
        "\n",
        "    # Фінальне порівняння стійкості\n",
        "    original_acc_res = results['ResNet-50']['Original']\n",
        "    original_acc_mob = results['MobileNetV2']['Original']\n",
        "\n",
        "    print(\"\\nПорівняння Падіння Точності ($\\Delta Accuracy$):\")\n",
        "    print(\"Тип Дефекту | ResNet-50 ($\\Delta$) | MobileNetV2 ($\\Delta$)\")\n",
        "    print(\"---|---|---\")\n",
        "\n",
        "    for defect in list(test_sets.keys())[1:]:\n",
        "        delta_res = original_acc_res - results['ResNet-50'][defect]\n",
        "        delta_mob = original_acc_mob - results['MobileNetV2'][defect]\n",
        "        print(f\"{defect: <15} | {delta_res:.4f} | {delta_mob:.4f}\")"
      ],
      "metadata": {
        "id": "Ic-jJE_tbZNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFVgu6EabcBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLisgZZ3df4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4LY_reCdfvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pK288gGdfoX",
        "outputId": "7d519f5d-21c8-43a1-af3f-4f02880e622b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m736.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python scikit-image -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akEn3lYs1mJE",
        "outputId": "8f052d6b-deb4-481c-b064-3a5377c74bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- КРОК 1: Ініціалізація та Завантаження ---\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from skimage.util import random_noise\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ВСТАВТЕ ВАШ АРХІВ СЮДИ: пр_10.zip\n",
        "# Розпакування архіву\n",
        "!unzip -q пр_10.zip\n",
        "print(\"Архів пр_10.zip успішно розпаковано. Дані готові.\")\n",
        "\n",
        "# --- Глобальні Параметри ---н\n",
        "DATA_DIR_TRAIN = './Train'\n",
        "DATA_DIR_TEST = './Test'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "# Змініть це, якщо ви працюєте з підмножиною!\n",
        "NUM_CLASSES = 5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OmSQlTRdfhE",
        "outputId": "a4120ce5-4a87-494a-a3d3-21603146a92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open пр_10.zip, пр_10.zip.zip or пр_10.zip.ZIP.\n",
            "Архів пр_10.zip успішно розпаковано. Дані готові.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"--- Початок реорганізації GTSRB (Виправлення шляху) ---\")\n",
        "\n",
        "REORG_TRAIN = './Train_reorg'\n",
        "REORG_TEST = './Test_reorg'\n",
        "os.makedirs(REORG_TRAIN, exist_ok=True)\n",
        "os.makedirs(REORG_TEST, exist_ok=True)\n",
        "class_info = {}\n",
        "\n",
        "# --- 2. Реорганізація НАВЧАЛЬНОГО набору ---\n",
        "print(\"Реорганізація навчального набору (Train)...\")\n",
        "try:\n",
        "    train_labels = pd.read_csv('Train.csv')\n",
        "\n",
        "    for index, row in tqdm(train_labels.iterrows(), total=len(train_labels), desc=\"Обробка Train\"):\n",
        "        class_id = str(row['ClassId']).zfill(2)\n",
        "        class_path = os.path.join(REORG_TRAIN, class_id)\n",
        "        os.makedirs(class_path, exist_ok=True)\n",
        "\n",
        "        # --- ВИПРАВЛЕННЯ ШЛЯХУ ---\n",
        "        # 1. Беремо шлях з CSV\n",
        "        original_relative_path = row['Path']\n",
        "        # 2. Видаляємо дублюючий префікс 'Train/'\n",
        "        if original_relative_path.startswith('Train/'):\n",
        "            original_relative_path = original_relative_path[len('Train/'):]\n",
        "\n",
        "        # 3. Формуємо правильний вихідний шлях\n",
        "        source_path = os.path.join('Train', original_relative_path)\n",
        "        destination_path = os.path.join(class_path, os.path.basename(row['Path']))\n",
        "\n",
        "        shutil.move(source_path, destination_path)\n",
        "        class_info[row['ClassId']] = class_info.get(row['ClassId'], 0) + 1\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ПОМИЛКА: Не знайдено файл Train.csv або папки Train. Перевірте архів.\")\n",
        "    raise\n",
        "\n",
        "# --- 3. Реорганізація ТЕСТОВОГО набору ---\n",
        "print(\"Реорганізація тестового набору (Test)...\")\n",
        "try:\n",
        "    test_labels = pd.read_csv('Test.csv')\n",
        "\n",
        "    for index, row in tqdm(test_labels.iterrows(), total=len(test_labels), desc=\"Обробка Test\"):\n",
        "        class_id = str(row['ClassId']).zfill(2)\n",
        "        class_path = os.path.join(REORG_TEST, class_id)\n",
        "        os.makedirs(class_path, exist_ok=True)\n",
        "\n",
        "        # --- ВИПРАВЛЕННЯ ШЛЯХУ ---\n",
        "        original_relative_path = row['Path']\n",
        "        # Тут шлях може починатися з 'Test/' (хоча в GTSRB частіше відсутній)\n",
        "        if original_relative_path.startswith('Test/'):\n",
        "            original_relative_path = original_relative_path[len('Test/'):]\n",
        "\n",
        "        source_path = os.path.join('Test', original_relative_path)\n",
        "        destination_path = os.path.join(class_path, os.path.basename(row['Path']))\n",
        "\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ПОМИЛКА: Не знайдено файл Test.csv або папки Test. Перевірте архів.\")\n",
        "    raise\n",
        "\n",
        "print(\"--- Реорганізація завершена! Тепер можна запускати Крок 2. ---\")\n",
        "\n",
        "# Обов'язково ПЕРЕЗАПУСТІТЬ ДРУГУ КЛІТИНКУ КОДУ,\n",
        "# переконавшись, що в ній DATA_DIR_TRAIN = './Train_reorg' та DATA_DIR_TEST = './Test_reorg'."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "XoH7DXT_gyi8",
        "outputId": "87c122a2-0437-420a-9896-9344e98c89e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Початок реорганізації GTSRB (Виправлення шляху) ---\n",
            "Реорганізація навчального набору (Train)...\n",
            "ПОМИЛКА: Не знайдено файл Train.csv або папки Train. Перевірте архів.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3942165689.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Реорганізація навчального набору (Train)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Обробка Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "print(\"\\n--- Фільтрування даних: Збереження лише 5 класів ---\")\n",
        "\n",
        "# --- 1. Визначення класів для збереження ---\n",
        "# ОБЕРІТЬ ВАШІ 5 КЛАСІВ (формат 'XX' як назва папки)\n",
        "# Приклад: Speed limit 20 (0), 30 (1), 50 (2), Stop (14), Yield (13)\n",
        "CLASSES_TO_KEEP = ['00', '01', '02', '13', '14']\n",
        "# Переконайтеся, що ви обрали ті 5, які вам потрібні!\n",
        "\n",
        "REORG_TRAIN = './Train_reorg'\n",
        "REORG_TEST = './Test_reorg'\n",
        "\n",
        "# --- 2. Фільтрування та видалення зайвих класів ---\n",
        "\n",
        "for data_dir in [REORG_TRAIN, REORG_TEST]:\n",
        "    print(f\"Обробка каталогу: {data_dir}\")\n",
        "\n",
        "    # Отримуємо список усіх папок класів у каталозі\n",
        "    all_class_dirs = [d for d in os.listdir(data_dir)\n",
        "                      if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "    # Визначаємо папки, які потрібно видалити\n",
        "    dirs_to_remove = [d for d in all_class_dirs if d not in CLASSES_TO_KEEP]\n",
        "\n",
        "    # Видалення зайвих папок\n",
        "    for class_id in dirs_to_remove:\n",
        "        path_to_remove = os.path.join(data_dir, class_id)\n",
        "        shutil.rmtree(path_to_remove)\n",
        "\n",
        "    print(f\"  Видалено {len(dirs_to_remove)} папок класів. Залишилося {len(CLASSES_TO_KEEP)}.\")\n",
        "\n",
        "# --- 3. Оновлення Глобальних Параметрів ---\n",
        "\n",
        "# ОНОВІТЬ ГЛОБАЛЬНІ ЗМІННІ ПЕРЕД ЗАПУСКОМ КРОКУ 2\n",
        "# (Ці зміни потрібно внести в початковий блок КРОК 2)\n",
        "# DATA_DIR_TRAIN = './Train_reorg' # Шляхи вже встановлені\n",
        "# DATA_DIR_TEST = './Test_reorg'\n",
        "NUM_CLASSES = len(CLASSES_TO_KEEP) # Тепер це 5\n",
        "\n",
        "print(f\"\\n✅ Фільтрування завершено. Встановлено NUM_CLASSES = {NUM_CLASSES}.\")\n",
        "print(\"Тепер можна безпечно запускати блок КРОК 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYoWWnNoirlv",
        "outputId": "7e9d3926-65c6-484f-b689-f4d4d59b6948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Фільтрування даних: Збереження лише 5 класів ---\n",
            "Обробка каталогу: ./Train_reorg\n",
            "  Видалено 38 папок класів. Залишилося 5.\n",
            "Обробка каталогу: ./Test_reorg\n",
            "  Видалено 38 папок класів. Залишилося 5.\n",
            "\n",
            "✅ Фільтрування завершено. Встановлено NUM_CLASSES = 5.\n",
            "Тепер можна безпечно запускати блок КРОК 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Глобальні Параметри ---\n",
        "DATA_DIR_TRAIN = './Train_reorg'\n",
        "DATA_DIR_TEST = './Test_reorg'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "# Змініть це, якщо ви працюєте з підмножиною!\n",
        "NUM_CLASSES = 5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "# --- КРОК 2: ПІДГОТОВКА ДАНИХ ТА АУГМЕНТАЦІЯ ---\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "def preprocess_dataset(ds, model_type='resnet'):\n",
        "    if model_type == 'resnet':\n",
        "        preprocess_fn = resnet_preprocess\n",
        "    elif model_type == 'mobilenet':\n",
        "        preprocess_fn = mobilenet_preprocess\n",
        "    else:\n",
        "        # Для сирого тестового набору\n",
        "        preprocess_fn = lambda x: x\n",
        "\n",
        "    if model_type == 'train':\n",
        "         ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "    return ds.map(lambda x, y: (preprocess_fn(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Завантаження та Розділення Train/Val\n",
        "full_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR_TRAIN, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "test_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR_TEST, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "CLASS_NAMES = full_train_ds.class_names\n",
        "val_size = int(len(full_train_ds) * VALIDATION_SPLIT)\n",
        "train_ds = full_train_ds.take(len(full_train_ds) - val_size)\n",
        "val_ds = full_train_ds.skip(len(full_train_ds) - val_size).take(val_size)\n",
        "\n",
        "# Фінальні набори даних\n",
        "train_ds_res = preprocess_dataset(train_ds, 'resnet')\n",
        "val_ds_res = preprocess_dataset(val_ds, 'resnet')\n",
        "test_ds_res = preprocess_dataset(test_ds_raw, 'resnet')\n",
        "train_ds_mob = preprocess_dataset(train_ds, 'mobilenet')  # <<< СТВОРЕННЯ train_ds_mob\n",
        "val_ds_mob = preprocess_dataset(val_ds, 'mobilenet')\n",
        "test_ds_mob = preprocess_dataset(test_ds_raw, 'mobilenet')\n",
        "\n",
        "print(f\"\\nПідготовка даних завершена. Використовуємо {len(CLASS_NAMES)} класів.\")\n",
        "\n",
        "\n",
        "# --- ФУНКЦІЇ ДЛЯ МОДЕЛЕЙ ---\n",
        "\n",
        "def create_model(base_model_class, input_shape, num_classes):\n",
        "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model, base_model\n",
        "\n",
        "def run_fine_tuning(model, base_model, ds_train, ds_val, ds_test, layers_to_unfreeze, model_name):\n",
        "    print(f\"\\n--- Fine-tuning {model_name} (Шари: {layers_to_unfreeze}) ---\")\n",
        "\n",
        "    # 1. Початкове навчання (5 епох)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    start_time_init = time.time()\n",
        "    hist_init = model.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
        "\n",
        "    # 2. Fine-tuning (10 епох)\n",
        "    base_model.trainable = True\n",
        "    if layers_to_unfreeze > 0:\n",
        "        for layer in base_model.layers[:-layers_to_unfreeze]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    hist_ft = model.fit(ds_train, epochs=10, validation_data=ds_val, verbose=0)\n",
        "\n",
        "    total_time = (time.time() - start_time_init)\n",
        "\n",
        "    # Оцінка\n",
        "    loss, acc = model.evaluate(ds_test, verbose=0)\n",
        "\n",
        "    return model, acc, total_time, hist_init.history, hist_ft.history\n",
        "\n",
        "# --- ВИКОНАННЯ ВСІХ ЕКСПЕРИМЕНТІВ FINE-TUNING ---\n",
        "\n",
        "FINAL_RESULTS = {}\n",
        "BEST_MODELS = {}\n",
        "\n",
        "# Експеримент 1: ResNet-50 (Fine-tuning 20 шарів)\n",
        "resnet_model, resnet_base = create_model(ResNet50, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "resnet_model_ft, acc_res, time_res, hist_res_init, hist_res_ft = run_fine_tuning(\n",
        "    resnet_model,\n",
        "    resnet_base,\n",
        "    train_ds_res,\n",
        "    val_ds_res,\n",
        "    test_ds_res,  # <<< ПЕРЕДАЄМО ds_test_res\n",
        "    20,\n",
        "    'ResNet50'      # <<< ПЕРЕДАЄМО 'ResNet50'\n",
        ")\n",
        "FINAL_RESULTS['ResNet50_20'] = {'acc': acc_res, 'time': time_res, 'params': resnet_model.count_params(), 'history': {'init': hist_res_init, 'ft': hist_res_ft}}\n",
        "BEST_MODELS['ResNet50'] = resnet_model_ft\n",
        "resnet_model_ft.save('ResNet50_best.h5')\n",
        "print(f\"ResNet50 (20 шарів) | Точність: {acc_res:.4f} | Час: {time_res:.2f} сек\")\n",
        "print(\"\\n--- ЗАВЕРШЕНО ResNet. ПОЧАТОК MobileNetV2 ---\") # <<<< ДОДАЙТЕ ЦЕЙ РЯДОК\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck5vv1J3djXr",
        "outputId": "2fd0546c-4bb3-4204-9a74-85294c428720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7620 files belonging to 5 classes.\n",
            "Found 2520 files belonging to 5 classes.\n",
            "\n",
            "Підготовка даних завершена. Використовуємо 5 класів.\n",
            "\n",
            "--- Fine-tuning ResNet50 (Шари: 20) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ПІДГОТОВКА СЕРЕДОВИЩА ТА ІМПОРТИ ---\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from skimage.util import random_noise\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import gc # <<< ІМПОРТ ДЛЯ ЗБИРАННЯ СМІТТЯ\n",
        "\n",
        "# --- ГЛОБАЛЬНІ ПАРАМЕТРИ (ВСТАНОВЛЕНІ ПІСЛЯ РЕОРГАНІЗАЦІЇ) ---\n",
        "DATA_DIR_TRAIN = './Train_reorg'\n",
        "DATA_DIR_TEST = './Test_reorg'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16 # <<< ЗМЕНШЕНО ДЛЯ ЕКОНОМІЇ ПАМ'ЯТІ\n",
        "NUM_CLASSES = 5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# --- ФУНКЦІЇ ПІДГОТОВКИ ДАНИХ ТА ПРЕПРОЦЕСИНГУ ---\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "def preprocess_dataset(ds, model_type='resnet'):\n",
        "    if model_type == 'resnet': preprocess_fn = resnet_preprocess\n",
        "    elif model_type == 'mobilenet': preprocess_fn = mobilenet_preprocess\n",
        "    else: preprocess_fn = lambda x: x\n",
        "    if model_type == 'train': ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    # Використовуємо .unbatch() та .batch() для уникнення проблем з попереднім кешуванням великого розміру пакета\n",
        "    return ds.map(lambda x, y: (preprocess_fn(x), y)).unbatch().batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# --- ЗАВАНТАЖЕННЯ ТА РОЗДІЛЕННЯ ДАНИХ ---\n",
        "\n",
        "try:\n",
        "    full_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        DATA_DIR_TRAIN, labels='inferred', label_mode='categorical',\n",
        "        image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=True\n",
        "    )\n",
        "    test_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "        DATA_DIR_TEST, labels='inferred', label_mode='categorical',\n",
        "        image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=False\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Помилка завантаження даних. Перевірте шляхи: {e}\")\n",
        "    raise e\n",
        "\n",
        "CLASS_NAMES = full_train_ds.class_names\n",
        "val_size = int(len(full_train_ds) * VALIDATION_SPLIT)\n",
        "train_ds = full_train_ds.take(len(full_train_ds) - val_size)\n",
        "val_ds = full_train_ds.skip(len(full_train_ds) - val_size).take(val_size)\n",
        "\n",
        "# Створюємо набори даних для ResNet\n",
        "train_ds_res = preprocess_dataset(train_ds, 'resnet')\n",
        "val_ds_res = preprocess_dataset(val_ds, 'resnet')\n",
        "test_ds_res = preprocess_dataset(test_ds_raw, 'resnet')\n",
        "\n",
        "# Створюємо набори даних для MobileNet (ці змінні поки залишаються в пам'яті, але будуть використані пізніше)\n",
        "train_ds_mob = preprocess_dataset(train_ds, 'mobilenet')\n",
        "val_ds_mob = preprocess_dataset(val_ds, 'mobilenet')\n",
        "test_ds_mob = preprocess_dataset(test_ds_raw, 'mobilenet')\n",
        "\n",
        "print(f\"\\nПідготовка даних завершена. Використовуємо {len(CLASS_NAMES)} класів.\")\n",
        "\n",
        "# --- ФУНКЦІЇ МОДЕЛЕЙ ТА FINE-TUNING ---\n",
        "\n",
        "def create_model(base_model_class, input_shape, num_classes):\n",
        "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model, base_model\n",
        "\n",
        "def run_fine_tuning(model, base_model, ds_train, ds_val, ds_test, layers_to_unfreeze, model_name):\n",
        "    print(f\"\\n--- Fine-tuning {model_name} (Шари: {layers_to_unfreeze}) ---\")\n",
        "\n",
        "    # 1. Початкове навчання класифікатора (3 епохи)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    start_time_init = time.time()\n",
        "    hist_init = model.fit(ds_train, epochs=3, validation_data=ds_val, verbose=0)\n",
        "\n",
        "    # 2. Fine-tuning (5 епох)\n",
        "    base_model.trainable = True\n",
        "    if layers_to_unfreeze > 0:\n",
        "        for layer in base_model.layers[:-layers_to_unfreeze]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    hist_ft = model.fit(ds_train, epochs=5, validation_data=ds_val, verbose=0)\n",
        "\n",
        "    total_time = (time.time() - start_time_init)\n",
        "    loss, acc = model.evaluate(ds_test, verbose=0)\n",
        "\n",
        "    return model, acc, total_time, hist_init.history, hist_ft.history\n",
        "\n",
        "# --- ВИКОНАННЯ ЕКСПЕРИМЕНТІВ ---\n",
        "\n",
        "FINAL_RESULTS = {}\n",
        "BEST_MODELS = {}\n",
        "\n",
        "# 1. ResNet-50\n",
        "resnet_model, resnet_base = create_model(ResNet50, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "resnet_model_ft, acc_res, time_res, hist_res_init, hist_res_ft = run_fine_tuning(\n",
        "    resnet_model, resnet_base, train_ds_res, val_ds_res, test_ds_res, 20, 'ResNet50'\n",
        ")\n",
        "FINAL_RESULTS['ResNet50_20'] = {'acc': acc_res, 'time': time_res, 'params': resnet_model.count_params(), 'history': {'init': hist_res_init, 'ft': hist_res_ft}}\n",
        "BEST_MODELS['ResNet50'] = resnet_model_ft\n",
        "resnet_model_ft.save('ResNet50_best.h5')\n",
        "print(f\"ResNet50 (20 шарів) | Точність: {acc_res:.4f} | Час: {time_res:.2f} сек\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# <<< СЕКЦІЯ ЗВІЛЬНЕННЯ ПАМ'ЯТІ ПЕРЕД НАВЧАННЯМ MOBILE NET >>>\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n!!! Звільняємо пам'ять ResNet50 перед MobileNetV2, щоб уникнути збою сесії !!!\")\n",
        "\n",
        "# Видаляємо ResNet моделі та набори даних ResNet-типу\n",
        "del resnet_model\n",
        "del resnet_base\n",
        "del resnet_model_ft\n",
        "del train_ds_res\n",
        "del val_ds_res\n",
        "del test_ds_res\n",
        "\n",
        "# Примусове очищення пам'яті\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Пам'ять ResNet50 звільнена. Перехід до MobileNetV2.\")\n",
        "\n",
        "\n",
        "# 2. MobileNetV2 (Модель буде створена зараз, використовуючи звільнену пам'ять)\n",
        "mobilenet_model, mobilenet_base = create_model(MobileNetV2, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "mobilenet_model_ft, acc_mob, time_mob, hist_mob_init, hist_mob_ft = run_fine_tuning(\n",
        "    mobilenet_model, mobilenet_base, train_ds_mob, val_ds_mob, test_ds_mob, 10, 'MobileNetV2'\n",
        ")\n",
        "FINAL_RESULTS['MobileNetV2_10'] = {'acc': acc_mob, 'time': time_mob, 'params': mobilenet_model.count_params(), 'history': {'init': hist_mob_init, 'ft': hist_mob_ft}}\n",
        "BEST_MODELS['MobileNetV2'] = mobilenet_model_ft\n",
        "mobilenet_model_ft.save('MobileNetV2_best.h5')\n",
        "print(f\"MobileNetV2 (10 шарів) | Точність: {acc_mob:.4f} | Час: {time_mob:.2f} сек\")\n",
        "\n",
        "\n",
        "# --- АНАЛІЗ: ENSEMBLE МОДЕЛЕЙ ---\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: ENSEMBLE МОДЕЛЕЙ ---\")\n",
        "# Оскільки ми видалили test_ds_res, нам потрібно тимчасово відновити його для прогнозу ResNet\n",
        "test_ds_res = preprocess_dataset(test_ds_raw, 'resnet')\n",
        "y_true = np.concatenate([y for x, y in test_ds_raw.unbatch().batch(BATCH_SIZE)]).argmax(axis=1)\n",
        "\n",
        "# Завантажуємо ResNet50 назад, оскільки ми його видалили\n",
        "BEST_MODELS['ResNet50'] = tf.keras.models.load_model('ResNet50_best.h5')\n",
        "\n",
        "preds_res = BEST_MODELS['ResNet50'].predict(test_ds_res, verbose=0)\n",
        "preds_mob = BEST_MODELS['MobileNetV2'].predict(test_ds_mob, verbose=0)\n",
        "\n",
        "# Середнє Агрегування\n",
        "preds_avg = (preds_res + preds_mob) / 2\n",
        "acc_ensemble = accuracy_score(y_true, np.argmax(preds_avg, axis=1))\n",
        "\n",
        "# Зважене Середнє\n",
        "w_res = FINAL_RESULTS['ResNet50_20']['acc']\n",
        "w_mob = FINAL_RESULTS['MobileNetV2_10']['acc']\n",
        "total_w = w_res + w_mob\n",
        "preds_weighted = (w_res / total_w * preds_res) + (w_mob / total_w * preds_mob)\n",
        "acc_weighted = accuracy_score(y_true, np.argmax(preds_weighted, axis=1))\n",
        "\n",
        "print(f\"Ensemble (Середнє) Точність: {acc_ensemble:.4f}\")\n",
        "print(f\"Ensemble (Зважене) Точність: {acc_weighted:.4f}\")\n",
        "FINAL_RESULTS['Ensemble_Avg'] = {'acc': acc_ensemble}\n",
        "\n",
        "# Звільнення пам'яті для подальших кроків\n",
        "del BEST_MODELS['ResNet50']\n",
        "del test_ds_res\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "# --- АНАЛІЗ: КВАНТИЗАЦІЯ (MobileNetV2) ---\n",
        "\n",
        "def measure_inference_speed(model, ds_test, model_name, interpreter=None):\n",
        "    images_to_test = 500\n",
        "    # Намагаємося уникнути використання .unbatch() для великих наборів, але для порівняння швидкості це потрібно\n",
        "    test_data_iter = ds_test.unbatch().take(images_to_test)\n",
        "\n",
        "    if interpreter:\n",
        "        input_details = interpreter.get_input_details()\n",
        "        input_index = input_details[0]['index']\n",
        "        start_time = time.time()\n",
        "        for x, _ in tqdm(test_data_iter, total=images_to_test, desc=f\"Inf {model_name}\"):\n",
        "            x_np = x.numpy()[None, ...].astype(input_details[0]['dtype'])\n",
        "            interpreter.set_tensor(input_index, x_np)\n",
        "            interpreter.invoke()\n",
        "    else:\n",
        "        start_time = time.time()\n",
        "        for x, _ in tqdm(test_data_iter, total=images_to_test, desc=f\"Inf {model_name}\"):\n",
        "            model.predict(x[None, ...], verbose=0)\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: КВАНТИЗАЦІЯ MOBILE NetV2 ---\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(BEST_MODELS['MobileNetV2'])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "quant_file_path = 'mobilenet_quantized.tflite'\n",
        "with open(quant_file_path, 'wb') as f: f.write(tflite_quant_model)\n",
        "\n",
        "original_size = os.path.getsize('MobileNetV2_best.h5') / (1024 * 1024)\n",
        "quant_size = os.path.getsize(quant_file_path) / (1024 * 1024)\n",
        "\n",
        "time_orig = measure_inference_speed(BEST_MODELS['MobileNetV2'], test_ds_mob, 'MobileNetV2 Keras')\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "time_quant = measure_inference_speed(None, test_ds_mob, 'MobileNetV2 TFLite', interpreter=interpreter)\n",
        "\n",
        "print(f\"Розмір Оригінал: {original_size:.2f} MB | Квантований: {quant_size:.2f} MB\")\n",
        "print(f\"Швидкість Keras: {time_orig:.4f} сек | TFLite Quant: {time_quant:.4f} сек (у {time_orig/time_quant:.1f}x швидше)\")\n",
        "FINAL_RESULTS['MobileNetV2_quant'] = {'size': quant_size, 'acc': FINAL_RESULTS['MobileNetV2_10']['acc'], 'inf_time': time_quant}\n",
        "\n",
        "\n",
        "# --- АНАЛІЗ: СТІЙКІСТЬ ДО ДЕФЕКТІВ ---\n",
        "\n",
        "def add_noise(image_tensor, mode='gaussian', var=0.01):\n",
        "    # Приймає tensor, повертає tensor\n",
        "    img_np = image_tensor.numpy() / 255.0\n",
        "    if mode == 'gaussian': sigma = var**0.5; noisy_img = img_np + np.random.normal(0, sigma, img_np.shape)\n",
        "    elif mode == 'salt_pepper': noisy_img = random_noise(img_np, mode='s&p', amount=var)\n",
        "    else: h, w = IMAGE_SIZE; M = np.float32([[1, 0.1, 0], [0, 1, 0]]); M[0, 2] = -0.1 * h / 2; noisy_img = cv2.warpAffine((img_np * 255).astype(np.uint8), M, (w, h), flags=cv2.INTER_LINEAR) / 255.0\n",
        "    return tf.convert_to_tensor(np.clip(noisy_img * 255.0, 0, 255).astype(np.float32))\n",
        "\n",
        "def create_defective_ds(defect_fn, preprocess_fn):\n",
        "    return test_ds_raw.unbatch().map(lambda x, y: (defect_fn(x), y), num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).map(lambda x, y: (preprocess_fn(x), y) )\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: СТІЙКІСТЬ ---\")\n",
        "defect_tests = {'Gaussian (0.01)': lambda x: add_noise(x, 'gaussian'),'Salt-Pepper (0.01)': lambda x: add_noise(x, 'salt_pepper'),'Affine Shear': lambda x: add_noise(x, 'affine'),}\n",
        "\n",
        "def evaluate_stability(model_name, preprocess_fn):\n",
        "    acc_results = {}\n",
        "    original_acc = FINAL_RESULTS[f'{model_name}_20' if 'ResNet' in model_name else f'{model_name}_10']['acc']\n",
        "\n",
        "    # Завантажуємо модель для тестування (оскільки ми їх видаляли)\n",
        "    model = tf.keras.models.load_model(f'{model_name}_best.h5')\n",
        "\n",
        "    for defect, fn in defect_tests.items():\n",
        "        ds_def = create_defective_ds(fn, preprocess_fn)\n",
        "        loss, acc = model.evaluate(ds_def, verbose=0)\n",
        "        acc_results[defect] = acc\n",
        "        print(f\" {model_name} на {defect}: {acc:.4f} ($\\Delta$ {original_acc - acc:.4f})\")\n",
        "\n",
        "    FINAL_RESULTS[f'{model_name}_stability'] = acc_results\n",
        "    del model # Знову видаляємо\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "evaluate_stability('ResNet50', resnet_preprocess)\n",
        "evaluate_stability('MobileNetV2', mobilenet_preprocess)\n",
        "\n",
        "\n",
        "# --- ФІНАЛЬНИЙ ВИВІД ТА МАТРИЦЯ ЗМІШУВАННЯ ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"             ФІНАЛЬНА ЗВІТНА ТАБЛИЦЯ\")\n",
        "print(\"=\"*50)\n",
        "print(f\"| Модель | Точність | Час (сек) | Параметри (M) | Розмір (MB) |\")\n",
        "print(\"|---|---|---|---|---|\")\n",
        "print(f\"| ResNet50 | {FINAL_RESULTS['ResNet50_20']['acc']:.4f} | {FINAL_RESULTS['ResNet50_20']['time']:.2f} | {FINAL_RESULTS['ResNet50_20']['params'] / 1e6:.2f} | - |\")\n",
        "print(f\"| MobileNetV2 | {FINAL_RESULTS['MobileNetV2_10']['acc']:.4f} | {FINAL_RESULTS['MobileNetV2_10']['time']:.2f} | {FINAL_RESULTS['MobileNetV2_10']['params'] / 1e6:.2f} | {original_size:.2f} |\")\n",
        "print(f\"| Ensemble Avg | {FINAL_RESULTS['Ensemble_Avg']['acc']:.4f} | - | - | - |\")\n",
        "print(f\"| MobNet Quant | {FINAL_RESULTS['MobileNetV2_10']['acc']:.4f} | {FINAL_RESULTS['MobileNetV2_quant']['inf_time']:.4f} | - | {quant_size:.2f} |\")\n",
        "\n",
        "# Завантажуємо ResNet для матриці (вона потрібна лише тут)\n",
        "BEST_MODELS['ResNet50'] = tf.keras.models.load_model('ResNet50_best.h5')\n",
        "y_pred_res = np.argmax(BEST_MODELS['ResNet50'].predict(preprocess_dataset(test_ds_raw, 'resnet'), verbose=0), axis=1)\n",
        "\n",
        "# Матриця Змішування (ResNet)\n",
        "cm = confusion_matrix(y_true, y_pred_res)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
        "plt.title('Confusion Matrix for Best ResNet-50')\n",
        "plt.show()\n",
        "\n",
        "# Графік Навчання (ResNet)\n",
        "def plot_training_history(history_init, history_ft, model_name):\n",
        "    full_loss = history_init['loss'] + history_ft['loss']\n",
        "    full_val_loss = history_init['val_loss'] + history_ft['val_loss']\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(full_loss, label='Train Loss')\n",
        "    plt.plot(full_val_loss, label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(FINAL_RESULTS['ResNet50_20']['history']['init'], FINAL_RESULTS['ResNet50_20']['history']['ft'], 'ResNet50')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "rJytTkdfwNJ-",
        "outputId": "1fd534a4-eeba-4f50-f4f1-6da87d31fd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:262: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:262: SyntaxWarning: invalid escape sequence '\\D'\n",
            "/tmp/ipython-input-3574921872.py:262: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  print(f\" {model_name} на {defect}: {acc:.4f} ($\\Delta$ {original_acc - acc:.4f})\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3574921872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Експеримент 2: MobileNetV2 (Fine-tuning 10 шарів)\n",
        "mobilenet_model, mobilenet_base = create_model(MobileNetV2, IMAGE_SIZE + (3,), NUM_CLASSES)\n",
        "mobilenet_model_ft, acc_mob, time_mob, hist_mob_init, hist_mob_ft = run_fine_tuning(\n",
        "    mobilenet_model,\n",
        "    mobilenet_base,\n",
        "    train_ds_mob,\n",
        "    val_ds_mob,\n",
        "    test_ds_mob,  # <<< ПЕРЕДАЄМО ds_test_mob\n",
        "    10,\n",
        "    'MobileNetV2'   # <<< ПЕРЕДАЄМО 'MobileNetV2'\n",
        ")\n",
        "FINAL_RESULTS['MobileNetV2_10'] = {'acc': acc_mob, 'time': time_mob, 'params': mobilenet_model.count_params(), 'history': {'init': hist_mob_init, 'ft': hist_mob_ft}}\n",
        "BEST_MODELS['MobileNetV2'] = mobilenet_model_ft\n",
        "mobilenet_model_ft.save('MobileNetV2_best.h5')\n",
        "print(f\"MobileNetV2 (10 шарів) | Точність: {acc_mob:.4f} | Час: {time_mob:.2f} сек\")\n",
        "\n",
        "\n",
        "# --- АНАЛІЗ: ENSEMBLE ---\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: ENSEMBLE МОДЕЛЕЙ ---\")\n",
        "y_true = np.concatenate([y for x, y in test_ds_raw]).argmax(axis=1)\n",
        "\n",
        "preds_res = BEST_MODELS['ResNet50'].predict(test_ds_res, verbose=0)\n",
        "preds_mob = BEST_MODELS['MobileNetV2'].predict(test_ds_mob, verbose=0)\n",
        "\n",
        "# Середнє Агрегування\n",
        "preds_avg = (preds_res + preds_mob) / 2\n",
        "acc_ensemble = accuracy_score(y_true, np.argmax(preds_avg, axis=1))\n",
        "\n",
        "# Зважене Середнє (Ваги пропорційні точності)\n",
        "w_res = FINAL_RESULTS['ResNet50_20']['acc']\n",
        "w_mob = FINAL_RESULTS['MobileNetV2_10']['acc']\n",
        "total_w = w_res + w_mob\n",
        "preds_weighted = (w_res / total_w * preds_res) + (w_mob / total_w * preds_mob)\n",
        "acc_weighted = accuracy_score(y_true, np.argmax(preds_weighted, axis=1))\n",
        "\n",
        "print(f\"Ensemble (Середнє) Точність: {acc_ensemble:.4f}\")\n",
        "print(f\"Ensemble (Зважене) Точність: {acc_weighted:.4f}\")\n",
        "FINAL_RESULTS['Ensemble_Avg'] = {'acc': acc_ensemble}\n",
        "\n",
        "\n",
        "# --- АНАЛІЗ: КВАНТИЗАЦІЯ ---\n",
        "\n",
        "def measure_inference_speed(model, ds_test, model_name, interpreter=None):\n",
        "    images_to_test = 500\n",
        "    test_data_iter = ds_test.unbatch().take(images_to_test)\n",
        "\n",
        "    if interpreter:\n",
        "        # Для TFLite\n",
        "        input_details = interpreter.get_input_details()\n",
        "        input_index = input_details[0]['index']\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        start_time = time.time()\n",
        "        for x, _ in tqdm(test_data_iter, total=images_to_test, desc=f\"Inf {model_name}\"):\n",
        "            x_np = x.numpy()[None, ...].astype(input_details[0]['dtype'])\n",
        "            interpreter.set_tensor(input_index, x_np)\n",
        "            interpreter.invoke()\n",
        "\n",
        "    else:\n",
        "        # Для Keras моделі\n",
        "        start_time = time.time()\n",
        "        for x, _ in tqdm(test_data_iter, total=images_to_test, desc=f\"Inf {model_name}\"):\n",
        "            model.predict(x[None, ...], verbose=0)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return elapsed_time\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: КВАНТИЗАЦІЯ MOBILE NetV2 ---\")\n",
        "\n",
        "# 1. Квантизація\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(BEST_MODELS['MobileNetV2'])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "quant_file_path = 'mobilenet_quantized.tflite'\n",
        "with open(quant_file_path, 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "original_size = os.path.getsize('MobileNetV2_best.h5') / (1024 * 1024)\n",
        "quant_size = os.path.getsize(quant_file_path) / (1024 * 1024)\n",
        "print(f\"Розмір Оригінал: {original_size:.2f} MB | Квантований: {quant_size:.2f} MB\")\n",
        "FINAL_RESULTS['MobileNetV2_quant'] = {'size': quant_size, 'acc': 0, 'inf_time': 0}\n",
        "\n",
        "# 2. Вимірювання швидкості\n",
        "time_orig = measure_inference_speed(BEST_MODELS['MobileNetV2'], test_ds_mob, 'MobileNetV2 Keras')\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "time_quant = measure_inference_speed(None, test_ds_mob, 'MobileNetV2 TFLite', interpreter=interpreter)\n",
        "\n",
        "print(f\"Швидкість Keras: {time_orig:.4f} сек | TFLite Quant: {time_quant:.4f} сек (у {time_orig/time_quant:.1f}x швидше)\")\n",
        "FINAL_RESULTS['MobileNetV2_quant']['inf_time'] = time_quant\n",
        "\n",
        "\n",
        "# --- АНАЛІЗ: СТІЙКІСТЬ ДО ДЕФЕКТІВ ---\n",
        "\n",
        "def add_noise(image_tensor, mode='gaussian', var=0.01):\n",
        "    img_np = image_tensor.numpy() / 255.0\n",
        "    if mode == 'gaussian':\n",
        "        sigma = var**0.5\n",
        "        noisy_img = img_np + np.random.normal(0, sigma, img_np.shape)\n",
        "    elif mode == 'salt_pepper':\n",
        "        noisy_img = random_noise(img_np, mode='s&p', amount=var)\n",
        "    else: # Affine Shear\n",
        "        h, w = IMAGE_SIZE\n",
        "        M = np.float32([[1, 0.1, 0], [0, 1, 0]]) # Shear factor 0.1\n",
        "        M[0, 2] = -0.1 * h / 2\n",
        "        noisy_img = cv2.warpAffine((img_np * 255).astype(np.uint8), M, (w, h)) / 255.0\n",
        "\n",
        "    return tf.convert_to_tensor(np.clip(noisy_img * 255.0, 0, 255).astype(np.float32))\n",
        "\n",
        "def create_defective_ds(defect_fn, preprocess_fn):\n",
        "    return test_ds_raw.unbatch().map(\n",
        "        lambda x, y: (defect_fn(x), y),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).batch(BATCH_SIZE).map(\n",
        "        lambda x, y: (preprocess_fn(x), y)\n",
        "    )\n",
        "\n",
        "print(\"\\n--- АНАЛІЗ: СТІЙКІСТЬ ---\")\n",
        "\n",
        "defect_tests = {\n",
        "    'Gaussian (0.01)': lambda x: add_noise(x, 'gaussian'),\n",
        "    'Salt-Pepper (0.01)': lambda x: add_noise(x, 'salt_pepper'),\n",
        "    'Affine Shear': lambda x: add_noise(x, 'affine'),\n",
        "}\n",
        "\n",
        "def evaluate_stability(model, model_name, preprocess_fn):\n",
        "    acc_results = {}\n",
        "    original_acc = FINAL_RESULTS[f'{model_name}_20' if 'ResNet' in model_name else f'{model_name}_10']['acc']\n",
        "\n",
        "    for defect, fn in defect_tests.items():\n",
        "        ds_def = create_defective_ds(fn, preprocess_fn)\n",
        "        loss, acc = model.evaluate(ds_def, verbose=0)\n",
        "        acc_results[defect] = acc\n",
        "        print(f\" {model_name} на {defect}: {acc:.4f} ($\\Delta$ {original_acc - acc:.4f})\")\n",
        "\n",
        "    FINAL_RESULTS[f'{model_name}_stability'] = acc_results\n",
        "\n",
        "evaluate_stability(BEST_MODELS['ResNet50'], 'ResNet50', resnet_preprocess)\n",
        "evaluate_stability(BEST_MODELS['MobileNetV2'], 'MobileNetV2', mobilenet_preprocess)\n",
        "\n",
        "\n",
        "# --- ФІНАЛЬНИЙ ВИВІД ТА МАТРИЦЯ ЗМІШУВАННЯ ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"             ФІНАЛЬНА ЗВІТНА ТАБЛИЦЯ\")\n",
        "print(\"=\"*50)\n",
        "print(f\"| Модель | Точність | Час (сек) | Параметри (M) | Розмір (MB) |\")\n",
        "print(\"|---|---|---|---|---|\")\n",
        "print(f\"| ResNet50 | {FINAL_RESULTS['ResNet50_20']['acc']:.4f} | {FINAL_RESULTS['ResNet50_20']['time']:.2f} | {FINAL_RESULTS['ResNet50_20']['params'] / 1e6:.2f} | - |\")\n",
        "print(f\"| MobileNetV2 | {FINAL_RESULTS['MobileNetV2_10']['acc']:.4f} | {FINAL_RESULTS['MobileNetV2_10']['time']:.2f} | {FINAL_RESULTS['MobileNetV2_10']['params'] / 1e6:.2f} | {original_size:.2f} |\")\n",
        "print(f\"| Ensemble Avg | {FINAL_RESULTS['Ensemble_Avg']['acc']:.4f} | - | - | - |\")\n",
        "print(f\"| MobNet Quant | {FINAL_RESULTS['MobileNetV2_10']['acc']:.4f} | {FINAL_RESULTS['MobileNetV2_quant']['inf_time']:.4f} | - | {quant_size:.2f} |\")\n",
        "\n",
        "\n",
        "# Матриця Змішування (ResNet)\n",
        "y_pred_res = np.argmax(preds_res, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred_res)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
        "plt.title('Confusion Matrix for Best ResNet-50')\n",
        "plt.show()\n",
        "\n",
        "# Графік Навчання (ResNet)\n",
        "def plot_training_history(history_init, history_ft, model_name):\n",
        "    full_loss = history_init['loss'] + history_ft['loss']\n",
        "    full_val_loss = history_init['val_loss'] + history_ft['val_loss']\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(full_loss, label='Train Loss')\n",
        "    plt.plot(full_val_loss, label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(FINAL_RESULTS['ResNet50_20']['history']['init'], FINAL_RESULTS['ResNet50_20']['history']['ft'], 'ResNet50')"
      ],
      "metadata": {
        "id": "ne_XKkJswNxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}